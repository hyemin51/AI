# rag_pipeline.py
import os
from dotenv import load_dotenv

# -------------------------------------------
# 0. í™˜ê²½ë³€ìˆ˜ ë¡œë“œ (.env ë˜ëŠ” Streamlit Secrets)
# -------------------------------------------
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# -------------------------------------------
# 1. LangChain ê´€ë ¨ import
# -------------------------------------------
from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate


# -------------------------------------------
# 2. ë¬¸ì„œ ë¡œë“œ í•¨ìˆ˜
# -------------------------------------------
def load_docs(docs_path="docs"):
    """docs í´ë” ì•ˆì˜ pdf/txt íŒŒì¼ì„ ëª¨ë‘ ì½ì–´ì„œ LangChain ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜"""
    docs = []

    # PDF íŒŒì¼
    pdf_loader = DirectoryLoader(docs_path, glob="*.pdf", loader_cls=PyPDFLoader)
    docs.extend(pdf_loader.load())

    # TXT íŒŒì¼
    txt_loader = DirectoryLoader(docs_path, glob="*.txt", loader_cls=TextLoader)
    docs.extend(txt_loader.load())

    return docs


# -------------------------------------------
# 3. ë¬¸ì„œ ì²­í¬ ë¶„í• 
# -------------------------------------------
def split_docs(documents, chunk_size=800, chunk_overlap=150):
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=["\n\n", "\n", ".", " ", ""],
    )
    return splitter.split_documents(documents)


# -------------------------------------------
# 4. ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ë° ì €ì¥
# -------------------------------------------
def build_vectorstore(chunks, save_path="vectorstore"):
    """ë¬¸ì„œ ì²­í¬ â†’ ì„ë² ë”© â†’ FAISS ë²¡í„°DB ì €ì¥"""
    embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)
    vectordb = FAISS.from_documents(chunks, embedding=embeddings)
    vectordb.save_local(save_path)
    return vectordb


# -------------------------------------------
# 5. ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ ë¶ˆëŸ¬ì˜¤ê¸°
# -------------------------------------------
def load_vectorstore(save_path="vectorstore"):
    """ì´ë¯¸ ë§Œë“¤ì–´ë‘” ë²¡í„°DBë¥¼ ë‹¤ì‹œ ë¡œë“œ"""
    embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)
    vectordb = FAISS.load_local(
        save_path,
        embeddings,
        allow_dangerous_deserialization=True,
    )
    return vectordb


# -------------------------------------------
# 6. ì§ˆë¬¸ â†’ ë‹µë³€ í•¨ìˆ˜ ìƒì„± (í”„ë¡¬í”„íŠ¸ í¬í•¨)
# -------------------------------------------
def make_answer_function(vectordb):
    """
    make_answer_function(...) -> answer_question(question: str) í˜•íƒœì˜ í•¨ìˆ˜ë¥¼ ë¦¬í„´.
    Streamlitì—ì„œ ë°”ë¡œ í˜¸ì¶œ ê°€ëŠ¥.
    """

    retriever = vectordb.as_retriever(search_kwargs={"k": 3})

    # ğŸ§  í”„ë¡¬í”„íŠ¸ (LLMì—ê²Œ ì¤„ ì§€ì‹œë¬¸)
    prompt = ChatPromptTemplate.from_template(
        """ë„ˆëŠ” ëŒ€í•™ìƒ ìˆ˜ì¤€ì˜ íšŒê³„/ì¬ë¬´ íŠœí„° ì±—ë´‡ì´ì•¼.
ì•„ë˜ì˜ ì°¸ê³  ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì¤˜.
ë¬¸ì„œ(context)ì— ì—†ëŠ” ë‚´ìš©ì€ "ê·¸ ë¶€ë¶„ì€ ìë£Œì— ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ì •ì§í•˜ê²Œ ë§í•´.
ì ˆëŒ€ ì¶”ì¸¡í•˜ê±°ë‚˜ ì§€ì–´ë‚´ì§€ ë§ˆ.

[ì°¸ê³  ë¬¸ì„œ]
{context}

[ì‚¬ìš©ì ì§ˆë¬¸]
{question}

ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê³  ëª…í™•í•˜ê²Œ ì„¤ëª…í•´ì¤˜."""
    )

    def answer_question(user_question: str) -> str:
        # (1) ë¬¸ì„œ ê²€ìƒ‰
        docs = retriever.get_relevant_documents(user_question)
        context_text = "\n\n".join([d.page_content for d in docs])

        # (2) LLM ì´ˆê¸°í™”
        llm = ChatOpenAI(
            api_key=OPENAI_API_KEY,
            model="gpt-4o-mini",
            temperature=0.2,
        )

        # (3) í”„ë¡¬í”„íŠ¸ ì±„ìš°ê¸°
        filled_prompt = prompt.format(context=context_text, question=user_question)

        # (4) LLM í˜¸ì¶œ
        response = llm.invoke(filled_prompt)

        # (5) í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜
        return getattr(response, "content", str(response))

    return answer_question

